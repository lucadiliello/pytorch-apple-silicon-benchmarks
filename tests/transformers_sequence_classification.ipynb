{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMweetwfS6oi7ZSMeRrCSNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devasworski/pytorch-apple-silicon-benchmarks/blob/master/transformers_sequence_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFeLGP5rLyYP",
        "outputId": "588a954d-0933-4543-fc26-45b377faf3d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 25 13:01:10 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import logging\n",
        "import torch\n",
        "from time import time\n",
        "from argparse import ArgumentParser\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from random import randint\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "Xb5Plt2yMoNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al8djtw7LxMM"
      },
      "outputs": [],
      "source": [
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "MODEL = \"bert-base-cased\" #@param [\"bert-base-cased\"]\n",
        "DEVICE = \"cuda\" #@param [\"cuda\", \"cpu\"]\n",
        "BATCH_SIZE = 256 #@param [\"16\", \"64\", \"256\"] {type:\"raw\"}\n",
        "MODE = \"training\" #@param [\"training\", \"inference\"]\n",
        "STEPS = 100 #@param [100]\n",
        "SEQUENCE_LENGHT = 512 #@param [\"128\", \"512\"] {type:\"raw\"}\n",
        "\n",
        "\n",
        "def train():\n",
        "\n",
        "    # instantiate model and tokenizer\n",
        "    tok = AutoTokenizer.from_pretrained(MODEL)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "    # get device\n",
        "    device = torch.device(DEVICE)\n",
        "\n",
        "    # move model to right device\n",
        "    model.to(device=device)\n",
        "\n",
        "    do_backprop = MODE == 'training'\n",
        "\n",
        "    # instantiate simple optimizer\n",
        "    if do_backprop:\n",
        "        optim = Adam(model.parameters(), lr=1e-04)\n",
        "\n",
        "    # create fake inputs (performance does not depend on the input tokens, just on the sequence length)\n",
        "    input_ids = [[randint(0, tok.vocab_size - 1) for _ in range(SEQUENCE_LENGHT)]] * BATCH_SIZE\n",
        "    attention_mask = [[1] * SEQUENCE_LENGHT] * BATCH_SIZE\n",
        "    labels = [randint(0, 1)] * BATCH_SIZE\n",
        "\n",
        "    # create input dict\n",
        "    inputs = dict(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "    # transform inputs in tensors\n",
        "    inputs = {k: torch.tensor(v) for k, v in inputs.items()}\n",
        "\n",
        "    logging.info(\"Input tensors size:\")\n",
        "    for k, v in inputs.items():\n",
        "        logging.info(f\" * {k}: {v.shape}\")\n",
        "\n",
        "    start_time = time()\n",
        "    for _ in tqdm(range(STEPS), desc=\"Testing...\", total=STEPS):\n",
        "        # move inputs to correct device\n",
        "        # cannot do it before because in a real-world scenario the data will be always different\n",
        "        data = {k: v.to(device=device) for k, v in inputs.items()}\n",
        "\n",
        "        if do_backprop:\n",
        "            optim.zero_grad()\n",
        "    \n",
        "        if do_backprop:\n",
        "            res = model(**data)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                res = model(**data)\n",
        "    \n",
        "        if do_backprop:\n",
        "            res.loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "    logging.info(f\"Model {MODEL} took {(time() - start_time):.2f} seconds to do {STEPS} steps in {MODE} with batch size {BATCH_SIZE} on {DEVICE}.\")\n",
        "train()"
      ]
    }
  ]
}
